# Goコードの記述と最適化

このドキュメントでは、高性能なGoコードを記述するためのベストプラクティスについて説明します。

現時点では、ビデオ、スライド、ブログ記事（「awesome-golang-performance」など）へのリンク集ですが、コンテンツが外部でなくここにある、より長い本形式に進化させるつもりです。リンクはカテゴリ別にソートする必要があります。

個々のサービスの高速化（キャッシングなど）についていくつか議論を行いますが、高パフォーマンスな分散システムの設計は範囲外です。まったく異なる一連の研究と設計のトレードオフを網羅しています。

すべてのコンテンツはCC-BY-SAの下でライセンスされます。

この本はつぎのセクションに分かれています。
   1) 遅くないソフトウェアを書くための基本的なヒント
     * CS 101-level のコンテンツ
   2) 高速なソフトウェアを書くためのヒント
     * Goの性能の引き出し方に関するGo固有のセクション
   3) *本当*に高速なソフトウェアを書くための高度なヒント
     * 最適化されたコードに高速化の余地がある場合

これらの3つのセクションを以下のように要約できます。
- 愚かになるな
- 賢くなれ
- 危険をおかせ

### いつ、どこを最適化するか

本当に最も重要なステップなので最初に書きます。あなたは最適化を行うべきなのでしょうか？

すべての最適化にはコストがかかります。一般的に、このコストはコードの複雑さか認知負荷の観点で表現されます。つまり、最適化されたコードは最適化されていないバージョンよりもシンプルであることは滅多にありません。

しかし、最適化の経済という別の側面があります。プログラマーとして、あなたの時間は貴重です。バグ修正や機能追加など、プロジェクトにおいて取り組む機会機会費用があります。最適化するのは楽しいですが、選択するのがしつも適切な仕事ではありません。パフォーマンスは機能ですが、リリースや品質もしかりです。

作業する最重要なものを選択するのです。それは時にはCPU最適化ではなく、ユーザーエクスペリエンスです。プログレスバーを追加するシンプルなものだったり、ページのレンダリング後にバックグラウンドで処理してページのレスポンスを改善することだったりします。

3時間後の毎時レポートが1時間もかからないレポートより役に立たないこともあるのです。

簡単に最適化できるからといっても最適化する価値があるわけではありません。容易に解決できる問題を無視することは有効な開発戦略です。

これを*あなたの*時間最適化だと考えてください。

何を最適化すべきかを選択し、いつ最適化すべきかを選択するのです。

「時期尚早な最適化」を定義してください。97%がそうですが、重要な3%に取り組んでください。

TPOP: 最適化すべきだろうか？「すべきだ。しかし、プログラムが本当に遅すぎ、問題が重要であるときに限る上に、正確さ、堅牢性、明快さを維持しつつ高速化されるという期待がある」

高速なソフトウェアか高速な開発です。

http://bitfunnel.org/strangeloop に数値があります。1台につき年1000USドルする機器が3万台必要な架空の検索エンジンを想像してください。ソフトウェアのスピードを倍増させることで年間1500万ドルを節約できます。1%削減するために開発者が1年取り組んでも割りに合うでしょう。

たいていの場合、プログラムのサイズとスピードは重要ではありません。至極単純な最適化は行う必要がありません。2番目に簡単な最適化は、より速いハードウェアを購入することです。

プログラムを変更しようと決めたら、続きを読んでください。

### 最適化方法

## 最適化の手順

詳細の前に、最適化の一般的なプロセスについて説明します。

最適化はリファクタリングの一種です。しかし、各ステップではソースコードのいくつかの側面（コードの重複、明快さなど）を改善するのではなく、パフォーマンスのいくつかの側面を改善します。たとえば、CPUやメモリ使用低減、レイテンシの低減などです。この改善は、一般的に可読性を犠牲にします。つまり、ユニットテストの包括的なセットに加え（変更が何かを壊していないことを保証するため）、変更がパフォーマンスに望ましい影響を与えているのを確認するために良いベンチマークセットが必要ということです。変更が実際にCPU使用を減少させていることを確認できる必要があります。改善すると思った変更は、実際にはゼロか負の影響を及ぼすこともあります。このような場合には必ず修正を元に戻してください。

https://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered

```go
//
// 親愛なる保守者様
//
// このルーチンを最適化しようとして
// それがひどい間違いだったと気づいたら
// 次の保守者への警告として下記のカウンターを
// インクリメントしてください
//
// total_hours_wasted_here = 42
//
```

使用しているベンチマークは正確で、代表的なワークロードで再現可能な数値を提供する必要があります。個別実行の分散が大きすぎると、小さな改善を見つけるのが難しくなります。あなたはベンチスタットかそれに相当する統計テストを使用する必要があり、そこから目を背けることはできません。（いずれにしても統計的なテストを使うのは良い考えです。）ベンチマークを実行する手順は文書化され、カスタムスクリプトとツールは実行方法の説明と一緒にリポジトリにコミットされているべきです。実行に時間がかかる大きなベンチマークスイートに注意してください。開発のイテレーションが遅くなります。

測定できるものはすべて最適化できることにも注意してください。正しいものを測定していることを確認してください。

次のステップは、最適化するものの決定です。目標がCPUを改善することであれば、許容できるスピードはどれくらいでしょうか。現在のパフォーマンスを2倍にしたいですか？10倍にしたいですか？「時間T未満のサイズNの問題」と捉えられますか？メモリ使用量を減らそうとしていますか？どれくらいですか？メモリ使用のなんの変化に対してどれだけ遅くなってもかまいませんか？小スペース化と引き換えに、なにを諦めますか？

サービスのレイテンシーのための最適化はより難しい提案です。Webサーバーのパフォーマンスをテストする方法については、書籍全体を通して書かれています。主な問題は、シングルスレッドコードの場合特定の問題サイズでパフォーマンスがかなり一定であることです。Webサービスの場合は単一値ではありません。適切なWebサービスベンチマークスイートは、特定のリクエスト/秒レベルのレイテンシ分布を提供します。（Gil Teneさんのお話へのリンクを貼る予定）

パフォーマンス目標は具体的でなければなりません。あなたは（ほとんど）より速いものを作ることができるでしょう。最適化はしばしば収穫逓減のゲームです。いつ止めるべきかを知る必要があります。仕事の最後のほんの少しを得るためにどれほどの努力を払うのでしょうか？どれほど醜くメンテナンスしにくいコードが書きたいのでしょうか？

ダン・ルーもまた目標パフォーマンス数値が妥当かどうかを判断するための大まかな計算の利点も指摘しています。

グリーンフィールド開発では、最後までベンチマーキングとパフォーマンスの数値を置き去りにすべきではありません。「後で修正する」と言うのは簡単ですが、パフォーマンスが本当に重要な場合は、最初から設計上の検討事項です。パフォーマンスの問題を修正するために必要なアーキテクチャ上の大幅な変更は、締め切り近くでは危険です。開発*期間中*は、合理的なプログラム設計、アルゴリズム、データ構造に焦点を当てるべきであることに注意してください。スタックの下位レベルでの最適化は、システムパフォーマンスのより完全な見通しがつく開発サイクルの後半まで待つべきです。システムが不完全な間にフルシステムプロファイルを取得しても、完成したシステムのどこにボトルネックの所在を見誤ります。

[幾千の傷による死](https://en.wikipedia.org/wiki/Death_by_a_Thousand_Cuts_(book))です。

ベンチマークできるコードを書いてください。大きなシステムではできる限りプロファイルしてください。テストしたい孤立した部分をベンチマークしてください。ベンチマークで十分にテストされ、代表的なものである十分なコンテキストを抽出して設定できる必要があります。

ターゲットと現在のパフォーマンスとの差異から、どこから始めるべきかを知ることができます。10％〜20％のパフォーマンス上が必要なだけの場合は、おそらくいくらかの実装の調整や軽微な修正で間に合うでしょう。10倍以上の最適化が必要な場合は、乗算を左シフトで置き換えるだけでは達成できません。おそらくあなたのスタックの変化が必要でしょう。

優れたパフォーマンスの作業には、システム設計、ネットワーキング、ハードウェア（CPU、キャッシュ、ストレージ）、アルゴリズム、チューニング、デバッグなど、さまざまなレベルの知識が必要です。限られた時間とリソースで、どのレベルが最も改善をもたらすかを検討してください。いつもアルゴリズムやプログラムのチューニングとは限りません。

この本では主にCPU使用、メモリ使用、レイテンシの低減を扱います。3つすべてを行うことは滅多にできないことを指摘しておきます。おそらく、CPU時間を高速化すると、プログラムはより多くのメモリを使用するでしょう。おそらく、メモリ空間を減らす必要があるとき、プログラムはより長い時間を要するでしょう。

アムダールの法則ではボトルネックに焦点を当てるべきとされています。ランタイムの5％しか必要としないルーチンの速度を2倍にしても、正味2.5%のスピードアップにしかなりません。一方、80%の時間を占めるルーチンを10%スピードアップさせるだけでも8%近く実行時間を改善できます。プロファイルは時間が実際に費やされた場所を特定するのに役立ちます。

最適化の際、CPUの仕事量を減らしたいとしましょう。同じ問題（ソート）をより少ないステップで解決するため、クイックソートはバブルソートよりも高速です。より効率的なアルゴリズムです。同じタスクを達成するためにCPUが行う必要がある作業を減らしました。

コンパイラの最適化と同様、プログラムのチューニングは通常、ランタイム全体でわずかな減少しかもたらしません。ほとんどの場合、アルゴリズムの変更やデータ構造の変更等、プログラムの構成方法の根本的な変更により効果の大きい最適化を実現できます。コンパイラ技術は向上していますが、スピードはゆっくりです。[Proebstingの法則](http://proebsting.cs.arizona.edu/law.html)ではコンパイラの性能は18年毎に倍になるとされており、これはプロセッサの性能は18カ月ごとに倍になるという（少し解釈が誤解されている）ムーアの法則と対照的です。アルゴリズムの改善は、より大きい規模で機能します。[混合整数プログラミングのアルゴリズムは、1991年から2008年の間に3万倍向上しました。](https://agtb.wordpress.com/2010/12/23/progress-in-algorithms-beats-moore%E2%80%99s-law/)より具体的な例として、Uberのブログ記事で説明されているブルートフォース地理空間アルゴリズムをより専門的なもの、よりタスクに適したものに置き換える詳細について考えてみましょう。 https://medium.com/@buckhx/unwinding-uber-s-most-efficient-service-406413c5871d

プロファイラは、特定のルーチンで多くの時間が費やされていることを示すことがあります。これは高コストのルーチンなのかもしれませんし、単に何度も呼ばれている低コストのルーチンかもしれません。すぐにその1つのルーチンをスピードアップしようとするのではなく、呼び出された回数を減らしたり、完全に無くしたりできるかどうかを確認してください。より具体的な最適化戦略については次のセクションで説明します。

最適化に関する質問が3つあります。

- 最適化する必要がありますか？最速のコードは決して実行されないコードです。
- もし必要があるなら、これは最良のアルゴリズムでしょうか？
- もし必要があるなら、これはそのアルゴリズムの最良の*実装*でしょうか？

### 具体的な最適化のヒント

Jon Bentleyの「効率的なプログラム記述」（1982年）ではエンジニアリングの問題としてプログラム最適化を取り扱いました。ベンチマーク。分析。改善。検証。反復。彼のヒントの多くは、現在ではコンパイラが自動的に実行します。プログラマーの仕事は、コンパイラーが*できない*変換を使用することです。

この本の要約です。
http://www.crowl.org/lawrence/programming/Bentley82.html
http://www.geoffprewett.com/BookReviews/WritingEfficientPrograms.html

プログラムチューニングのルールです。
https://web.archive.org/web/20080513070949/http://www.cs.bell-labs.com/cm/cs/pearls/apprules.html

プログラムの変更を考える際、2つの基本的な選択肢があります。データを変えるかコードを変えるかです。

## データの変更

データを変更するとは、処理しているデータの表現を追加したり変更したりするということです。

（これらの中には、データ構造のさまざまな側面に関連するO()の変更に依存するものがあります）

データ構造を拡張するためのアイデア:

- 追加のフィールド: たとえば、リンクトリストのサイズは必要なときにイテレーションするのではなく格納してください。もしくは頻繁に必要とされる他のノードへの追加のポインタを複数の検索に格納してください（たとえば二十リンクトリストでO(1)を削除するための「後方」リンク）。このような変更は、必要なデータの保存と最新の状態の維持が低コストな場合に役立ちます。

- 追加の検索インデックス: ほとんどのデータ構造は、単一タイプのクエリ用に設計されています。2つの異なるクエリタイプが必要な場合は、データに「ビュー」を追加することで大幅な改善が可能です。たとえば、IDによって参照される[]structやときにはmap[string]idのstring（もしくは*struct）

- 追加の要素情報: たとえばブルームフィルタです。これらは、データ構造の残りの部分を圧倒しないように、小さくて速い必要があります。

- クエリが高コストな場合は、キャッシュを追加してください。私たちはmemcacheに精通していますが、インプロセスキャッシュが存在します。
  * ワイヤー越しのネットワークとシリアライゼーションのコストは大きいです
  * インプロセスキャッシュでは失効を気にする必要があります。
  * 単一の項目でも役立ちます（たとえばログファイルの時間解析）

TODO: 「キャッシュ」はキーバリューではなく、作業していた場所へのポインタにすぎません。これは「サーチフィンガー」と同じくらいシンプルになり得ます。

これらはすべて、データ構造レベルで「仕事を減らす」という明確な例です。すべて空間コストがかかります。ほとんどの場合、CPUを最適化すると、プログラムはより多くのメモリを使用します。これは古典的な時間と空間のトレードオフです。
https://en.wikipedia.org/wiki/Space%E2%80%93time_tradeoff

プログラムがあまりにも多くのメモリを使用している場合は、逆の方法を取ることもできます。計算量の増加と引き換えに空間の使用量を減らすのです。事柄を保存するのではなく、毎回計算します。メモリ内にデータを圧縮し、必要に応じて即座に圧縮解除することもできます。

プログラムが使用する空間を減少させるテクニックをカバーした本をオンラインで利用できます。もともと組み込み開発者を対象に書かれていましたが、膨大な量のデータを処理する最新のハードウェア上のプログラムにも当てはまります。
http://www.smallmemory.com/

データを並べ替えてください。パディングを削除するのです。追加のフィールドを削除してください。遅いデータ構造に変更してください。ポインタの重いツリー構造をスキップし、代わりにスライスと線形検索を使用してください。データの圧縮方法をカスタマイズしてください。浮動小数点（go-tsz）や整数（deltaやxorとhuffman）などが利用できます。

データレイアウトについては後で詳しく説明します。

現代のコンピュータとメモリ階層は、空間と時間のトレードオフをあまり明確にしません。メモリ上で「遠く離れている」（それゆえアクセスにコストがかかる）テーブルの探索は
必要なときに再計算するだけで速くなります。

これはまた、キャッシュ競合により本番環境では実現されない改善が頻繁に見られることを意味します。（たとえば、テーブル探索はベンチマーク時にプロセッサキャッシュに格納されますが、実際のシステムで使用する場合は常に「実データ」でフラッシュされます。GoogleのJump Hash論文では、プロセッサキャッシュの満たすものと満たさないものの両方を比較しながらこれを直接的に述べています。Jump Hash論文のグラフ4と5を見てください https://arxiv.org/pdf/1406.2294.pdf ）

TODO: コンテンツキャッシュをシミュレートする方法、増分コストを表示する方法

考慮すべき別の側面は、データ転送時間です。一般的に、ネットワークとディスクへのアクセスは非常に遅いため、圧縮されたチャンクを読み込めるようになると、取得されたデータを解凍するのに必要なCPU時間よりもずっと高速になります。ここで、いつものようにベンチバークが重要です。バイナリ形式は一般的にテキスト形式よりも小さく、高速に解析することができますが、人間は読めなくなってしまいます。

## アルゴリズムの変更

データを変更する場合、別の主な選択肢はコードの変更です。

アルゴリズムを変更すると最大限の改善が見込めます。これはバブルソートをクイックソートでO（n^2）ソートからO（n log n）ソートに、または小さなO(n)だった配列を使った線形探索をマップ探索（O(1)）と置き換えるのと同じです。

こうしてソフトウェアは遅くなります。もともとある目的で設計された構造は別の目的で設計されたものに再利用されます。これは段階的に起こります。

異なるビッグオーレベルを直感的に理解することは重要です。問題に適したデータ構造を選択してください。いつも必要なわけではありませんが、遅くまで気づかれないかもしれない酷いパフォーマンス問題を防止できます。

複雑さの基本的な分類はつぎのとおりです。

* O(1): フィールドアクセス、配列、マップ探索
  アドバイス: 気にしなくてかまいません。
* O(log n): 二分探索
  アドバイス: ループ内でのみ問題です。
* O(n): シンプルなループ
  アドバイス: 日常的に行っています。
* O(n log n): 分割統治法、ソート
  アドバイス: まだ十分速いです。
* O(n*m): 入れ子ループ
  アドバイス: 注意し、集合のサイズを制約してください。
* 二次関数と部分指数関数の間のその他
  アドバイス: 百万行のデータがあるときは実行しないでください。
* O(b ^ n), O(n!): 指数関数的なもの以上
  アドバイス: たくさんのデータポイントがあるといいですね。

リンク: http://bigocheatsheet.com

ソートされていないデータの集合を検索する必要があるとしましょう。O(n)線形探索より高速なO（log n）二分探索を知っていて「二分探索を使うべきだ」と考えました。しかし、二分探索はデータがソートされている必要があります。つまり、O(n log n)の時間がかかるソートを最初に行う必要があります。たくさんの検索が必要であれば前段でソートするコストに見合います。一方、もし探索を行うのが大半の場合はおそらく配列を保持するのは誤りで、代わりにマップのO（1）探索コストを割く方がよいでしょう。

最も単純で合理的なデータ構造を選択し、移行してください。CS101は「遅くないソフトウェア」を書くことについてです。バカな真似はしてはいけません。これは開発時普段から心がけねばなりません。ランダムアクセスが必要が場合はリンクトリストを選択してはいけません。通りがけ順トラバーサルが必要な場合はマップを使用してはいけません。要件は変わルものなので、いつも未来を推測することはできません。ワークロードを適切に推測してください。

http://daslab.seas.harvard.edu/rum-conjecture/

似たような問題に対するデータ構造でも、個々の作業を行うときには異なるでしょう。挿入が行われると、二分木が少しずつそーとします。ソートされていない配列は挿入する方が速いですが、ソートされていません。最終的に「確定する」ためにはすべて同時にソートする必要があります。

他の人が使用するパッケージを書くときは、ユースケースごとにフロントを最適化するという誘惑に負けないでください。コードが読みづらくなるでしょう。設計によるデータ構造は単一目的であると効率的です。心を読むことも未来を予測することもできません。もしユーザが「あなたのパッケージはこのユースケースでは遅すぎる」と言うのであれば、「それなら別のパッケージを使ってください」と答えるのが合理的です。パッケージは「1つのことをうまくやる」べきです。

時には、ハイブリッドなデータ構造によって必要とするパフォーマンスの向上がもたらされることがあります。たとえば、データをバケツに入れることで検索を単一のバケツに制限できます。この方法は依然としてO（n）の理論的コストを割きますがが、定数は小さくなります。プログラムをチューニングするときには、このような調整をもう一度行います。

ビッグオー記法を議論する際、忘れがちなことが2つあります。

1つめは、関係する定数係数です。アルゴリズムの複雑さが同じ2つのアルゴリズムは、異なる定数係数を持ち得ます。リストを100回ループするのとたった1度だけループするのを想像してください。どちらもO(n)ですが、前者は100倍の定数係数を持ちます。これらの定数係数がたとえマージソート、クイックソート、ヒープソートなどO(n log n)であってもすべてクイックソートを使用する理由で、もっとも速いからです。クイックソートが最小の定数係数を持っています。

2つめは、ビッグオーでは「nが無限に向かって大きくなっていくにつれて」としかされていないことです。「数が大きくなるにつれ、実行時間を締める成長係数になる」という増加傾向ついての話です。実際のパフォーマンスや小さいnでどのように振る舞うかについては言及されていません。

ダンベルアルゴリズムの方が高速である分岐点が頻繁にあります。Go標準ライブラリの`sort`パッケージの良い例です。たいていの場合はクイックソートを使用しますが、パーティションサイズが12要素未満のときはシェルソートパスと挿入ソートが行われます。

いくつかのアルゴリズムにとって、定数係数はとても大きいのでこの分岐点はすべての有効な入力よりも大きくなります。つまり、O(n^2)アルゴリズムは扱うすべての入力に対するO(n)アルゴリズよりも高速です。

これはまた逆の状況にもなりますたとえば、小さな入力に対するベンチマークが遅くなってもO(n^2)でなくO(n)のスケーリングのためにより複雑なデータ構造を使用するなどです。これはほとんどのロックのないデータ構造にも当てはまります。一般的にシングルスレッドのケースでは遅いですが、多くのスレッドを伴う場合はよりスケールします。

現代のコンピュータのメモリ階層は、ここで少し問題を混乱させます。キャッシュにおいては効率的にポインタを追うランダムアクセスよりもスライスをスキャンする予測可能なアクセスの方が好まれます。やはりよいアルゴリズムで始めるのがベストです。これについては、ハードウェア固有のセクションで説明します。

> 試合の勝利は最強の者の手にわたるとは限らないし、競争の勝利は最速の者の手にわたるとも限らない。掛け方次第である。 -- ラドヤード・キップリング

ときに特定の問題に対する最良のアルゴリズムは単一のアルゴリズムではなく少し異なる入力クラスに特化したアルゴリズムの集まりです。この「ポリアルゴリズム」は、対処する必要がある入力の種類をすばやく検出し、適切なコードパスにディスパッチします。これはパッケージをソートする先述のもので、問題のサイズを決定し、異なるアルゴリズムを選択します。クイックソート、シェルソート、および挿入ソートの組み合わせに加えて、クイックソートの再帰深度も追跡し、必要に応じてヒープソートを呼び出します。`string`パッケージと`bytes`パッケージは似たような動作をし、異なるケースの検出と特化を行います。データ圧縮と同様に、入力の性質を知れば知るほど、カスタムソリューションはよくなっていきます。たとえ最適化がいつも適用できるわけではなくても、使用は安全であると決定し、異なるロジックを実行することは価値があります。

### ベンチマークの入力値

入力値のサイズが本番環境でどれくらいの大きさになりそうか知っておいてください。

ベンチマークには適切なサイズの入力を使用しなければいけません。これまで見てきたように、異なるアルゴリズムは異なる入力サイズに対して意味をなします。もし100未満の入力値を想定するのならばベンチマークにも反映するべきです。さもないと、n=10^6に最適化されたアルゴリズムが最速にならないかもしれません。

代表的なテストデータを生成してください。データの分布が異なるとアルゴリズムにおいて挙動が変わることがあります。たとえば、古典的な「データがソートされて入ればクイックソートはO(n^2)」ということを思い浮かべてください。同様に、一定のランダムデータに対する補間検索はO(log log n)ですが、最悪の場合O(n)になります。入力がどのようになるかを知っておくことは代表的なベンチマークと最適なアルゴリズム選択の両者にとっての鍵となります。たとえテストに使用しているデータが実際のワークロードを代表するようなものでい場合、容易にある特定のデータセットに最適化し、ある特定の入力値セットにだけ対してよく機能するコードに「過剰適合」させることになってしまうでしょう。

これはまた、ベンチマークデータは現実世界を代表するものである必要があるということです。もし、繰り返しリクエストされることが十分まれであるとすると、再計算よりも保持する方が高コストでしょう。もしベンチマークデータが同じような繰り返しのリクエストだけで構成されるならば、キャッシュのパフォーマンスを見誤るでしょう。

TODO: ランダムな入力値は効果的なようで現実世界の入力値を代表するものでない（ランダムグラフ、ランダムツリーなど）

また、ラップトップではわからなかった問題が、本番環境にデプロイし、40コアサーバーで250kリクエスト/秒に達するとわかることがあります。

よいベンチマークを書くことは難しいかもしれません。

TODO: 局所的なベンチマークでは遅くなったように見えるが、大局的（現実世界での）パフォーマンスは改善するケース

* https://timharris.uk/misc/five-ways.pdf
